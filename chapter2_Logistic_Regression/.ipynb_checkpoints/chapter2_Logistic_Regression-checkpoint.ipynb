{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew Ng机器学习（二）：Logistic回归和正则化\n",
    "机器学习公开课笔记以及作业题分析\n",
    "\n",
    "***\n",
    "## 逻辑回归\n",
    "### 假说模型\n",
    "逻辑回归虽然叫回归，但是是用来做分类问题的，y是离散的值\n",
    "\n",
    "先从二元逻辑回归开始，y取值为两种情况0和1\n",
    "\n",
    "假如依然采取线性模型，即根据是否大于0.5做分类，那么可能出现x的取值使y大于1或者小于0的情况出现\n",
    "\n",
    "因此在逻辑回归中，我们需要引入新的模型\n",
    "$$h(x)=g(\\theta^Tx)$$\n",
    "\n",
    "其中的g代表逻辑函数（logistic function），其中最常用的一种叫做S 形函数（Sigmoid function），其定义如下\n",
    "$$g(z)=\\frac1{1+e^{-z}}$$\n",
    "\n",
    "合起来就可以得到用于逻辑回归的模型\n",
    "$$h(x)=\\frac1{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "此时h(x)给出的结果是x对应的输出y=1的概率，也就是\n",
    "$$h(x)=P(y=1|x;\\theta)$$\n",
    "\n",
    "***\n",
    "### 决策边界\n",
    "模型中的分界线，将预测为1的区域和预测为0的趋于分成两部分\n",
    "\n",
    "***\n",
    "### 代价函数\n",
    "如果采用和线性模型相同的代价函数\n",
    "$$J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac1{2m}\\displaystyle{\\sum^m_{i=1}}(h(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "但是逻辑回归中，h(x)的形式变了。如果沿用上述公式，将导致J()是一个非凸函数，不利于我们找最值。因此，代价函数也需要重新定义\n",
    "$$J(\\theta)=\\frac1m\\displaystyle{\\sum^m_{i=1}}Cost(h(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "其中\n",
    "$$Cost(h(x),y)=\\left\\{\n",
    "\\begin{aligned}\n",
    "-log(h(x)), if \\quad y = 1 \\\\\n",
    "-log(1-h(x)), if \\quad y = 0\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "\n",
    "为了把Cost从两种情形合并为一种，可以写成如下的形式\n",
    "$$J(\\theta)=-\\frac1m\\displaystyle{\\sum^m_{i=1}}(y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)})))$$\n",
    "\n",
    "使用梯度下降算法求使代价函数最小的参数值\n",
    "$$\\theta_j=\\theta_j-\\alpha\\frac{d}{d\\theta_j}J(\\theta)$$\n",
    "\n",
    "对J求导后得到（需要推到）\n",
    "$$\\theta_j=\\theta_j-\\frac{\\alpha}m\\displaystyle{\\sum_{i=1}^m}((h(x^{(i})-y^{(i)})x^{(i)}_j)$$\n",
    "\n",
    "***\n",
    "### 多元分类问题\n",
    "由于逻辑回归只能处理2元分类的问题\n",
    "\n",
    "***\n",
    "## 正则化\n",
    "### 过拟合\n",
    "拟合问题\n",
    "![](https://pic2.zhimg.com/c278dcb3c42ff6b9c7d914b85030d77d_r.jpg)\n",
    "第一个用线性来拟合，显然不够准确，这称为欠拟合；第二个看上去刚刚好；第三个用过高次数的多项式，虽然完美拟合了训练集中的每个数据，但显然预测新数据时并不可靠，这称为过拟合\n",
    "\n",
    "欠拟合的解决方法是增加特征量\n",
    "\n",
    "过拟合的解决方法是减少特征量或者正则化\n",
    "\n",
    "***\n",
    "### 正则化代价函数\n",
    "$$J(\\theta)=\\frac1{2m}\\displaystyle{(\\sum_{i=1}^m}(h(x^{(i})-y^{(i)})^2+\\lambda\\displaystyle{\\sum_{j=1}^n\\theta_j^2})$$\n",
    "\n",
    "***\n",
    "### 线性回归的正则化\n",
    "\n",
    "***\n",
    "### 逻辑回归的正则化\n",
    "\n",
    "***\n",
    "### 参考\n",
    "[1]. [Andrew Ng机器学习课程笔记3——逻辑回归和正则化](http://www.yalewoo.com/andrew_ng_machine_learning_notes_3_logistic_regression_and_regularization.html)\n",
    "\n",
    "[2]. [机器学习公开课笔记(3)：Logistic回归](http://www.cnblogs.com/python27/p/MachineLearningWeek03.html)\n",
    "\n",
    "[3]. [第三周笔记：逻辑回归及正则化](https://zhuanlan.zhihu.com/p/21378251)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
