{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew Ng机器学习（三）：神经网络：表达\n",
    "机器学习公开课笔记以及作业题分析\n",
    "\n",
    "***\n",
    "## 神经网络\n",
    "### 引入\n",
    "\n",
    "在假设函数中$h(x)=g(\\theta^TX)$，对于非线性问题需要构造许多高次项，导致特征学习参数过多，复杂度高，计算量大，所以引入神经网络的模型\n",
    "\n",
    "***\n",
    "### 神经网络模型\n",
    "![](http://7d9rd6.com1.z0.glb.clouddn.com/wp-content/uploads/2016/04/scrn20160406193450.png)\n",
    "上图是一个3层的神经网络，从左到右依次是第1层，第2层，第3层。第1层就是输入，因此叫做输入层（input layer）；最后一层是输出，叫做输出层（output layer）。中间的叫做隐藏层（Hidden layer）。每一层都有一个额外添加的偏差单位（图中下标为0的结点，它们的输出值总是1）。\n",
    "图中每个结点类似于一个神经元，叫做激活单元（activation unit）。它的上标代表第几层，输入层也可以看做第1层激活单元，例如图中x1也可以看做$a^1_1$，x2也可以看做$a^1_2$。每个结点接收多个输入，有一个输出。 单个的结点的计算方法和逻辑回归中的计算方法相同\n",
    "key words：输入层， 隐藏层， 输出层， 激活单元， 偏差单位\n",
    "***\n",
    "### 正向传播\n",
    "在上图的神经网络模型中，计算从第一层到第二层：\n",
    "$$\n",
    "{\\left[\n",
    "\\begin{array}{}\n",
    "a_1^{(2)} \\\\\n",
    "a_2^{(2)} \\\\\n",
    "a_3^{(2)} \\\\\n",
    "\\end{array}\n",
    "\\right]}\n",
    "{=}\n",
    "g(\n",
    "{\\left[\n",
    "\\begin{array}{}\n",
    "\\theta_{10}^{(1)} & \\theta_{11}^{(1)} & \\theta_{12}^{(1)}& \\theta_{13}^{(1)} \\\\\n",
    "\\theta_{20}^{(1)} & \\theta_{21}^{(1)} & \\theta_{22}^{(1)} & \\theta_{23}^{(1)} \\\\\n",
    "\\theta_{30}^{(1)} & \\theta_{31}^{(1)} & \\theta_{32}^{(1)} & \\theta_{33}^{(1)} \\\\\n",
    "\\end{array}\n",
    "\\right]}\n",
    "\\times\n",
    "{\\left[\n",
    "\\begin{array}{}\n",
    "a_0^{(1)} \\\\\n",
    "a_1^{(1)} \\\\\n",
    "a_2^{(1)} \\\\\n",
    "a_3^{(1)} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "})\n",
    "$$\n",
    "向量化的计算方法：\n",
    "$$a^{(2)}=g(\\theta^{(1)}\\times a^{(1)})$$\n",
    "记$z^{(2)}=\\theta^{(1)}\\times a^{(1)}$，则有$a^{(2)}=g(z^{(2)})$，同理可以得到$h_\\theta(x)=a^{(3)}=g(z^{(3)})$\n",
    "\n",
    "***\n",
    "### 逻辑运算\n",
    "为何神经网络可以分类：\n",
    "（1）逻辑与\n",
    "利用S函数在z大于10后接近1，小于10后接近0这一特性设计权重\n",
    "$$h_\\theta(x)=g(-30+20x_1+20x_2)$$\n",
    "![](http://images2015.cnblogs.com/blog/353956/201512/353956-20151223094529984-616170583.png)\n",
    "（2）逻辑或\n",
    "![](http://images2015.cnblogs.com/blog/353956/201512/353956-20151223094752015-250805061.png)\n",
    "（3）逻辑非\n",
    "![](http://images2015.cnblogs.com/blog/353956/201512/353956-20151223211654468-561696106.png)\n",
    "（4）同或（异或非）\n",
    "![](http://images2015.cnblogs.com/blog/353956/201512/353956-20151223214757187-836059596.png)\n",
    "\n",
    "\n",
    "***\n",
    "### 多元分类问题\n",
    "![多元分类问题](https://raw.githubusercontent.com/mancunian100/Andrew_Ng_MachineLearning/master/chapter3_Neural_Networks/scrn20160407122653.png)\n",
    "\n",
    "采用多个输出单元表示每一种情况的输出\n",
    "\n",
    "***\n",
    "### 作业题分析\n",
    "（1）继续上一节的逻辑回归处理多元分类问题，采用one-vs-all方法，即对每种情况都单独作为二分类问题计算一遍，取最后输出中概率最大的情况作为输出情况，one-vs-all部分输出最大概率的代码：\n",
    "\n",
    "% ==== Pridiction of one-vs-all Classifier ====\n",
    "\n",
    "[Y, p(:)] = max(sigmoid(X*all_theta'), [], 2);\n",
    "\n",
    "（2）这一部分的神经网络是在已经给训练好的参数$\\theta$来计算多分类问题，也是采用one-vs-all方法，这部分代码：\n",
    "\n",
    "% ==== Compute p ====\n",
    "\n",
    "X_1 = [ones(m, 1) X];\n",
    "\n",
    "[y, p] = max(sigmoid([ones(m, 1), sigmoid(X_1*Theta1')]*Theta2'), [], 2);\n",
    "\n",
    "***\n",
    "### 参考\n",
    "[1]. [第四周笔记：神经网络是什么](https://zhuanlan.zhihu.com/p/21423252)\n",
    "\n",
    "[2]. [Andrew Ng机器学习课程笔记4——神经网络：表达](http://www.yalewoo.com/andrew_ng_machine_learning_notes_4_neural_networks_representation.html)\n",
    "\n",
    "[3]. [机器学习公开课笔记(4)：神经网络(Neural Network)——表示](http://www.cnblogs.com/python27/p/MachineLearningWeek04.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
